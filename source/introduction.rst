############
Introduction
############

This guide aims to give you any clue to go on working on our project. It will focus on technical choices and the technical aspects of the project while we describe the scientific aspects of the project within our project report.

=========================================
How to get any information on our project
=========================================

Project outputs
---------------

The outputs of our initial project are split into 3 main deliveries :

* Our source code, on github : `https://github.com/prBigData <https://github.com/prBigData>`_
* Our report, also available on github : `https://github.com/prBigData/report <https://github.com/prBigData/report>`_
* And the documentation you are reading right now


Where is what ?
---------------

Generally speaking, we have tried to describe our scientific approach as well as all the development steps we've been through within our report.

Then we give every technical clue in this documentation.

Finally our code is obviously on github.

More specifically, here are some details on the splitting :


* **Scraper thinking & development steps** : report
* **Scraper's algorithmics** : report and documentation
* **Scraper setup & launch** : documentation
* **Big data architecture thinking and development steps** : report
* **Big data architecture setup & launch** : documentation
* **Data analysis thinking & development** : report
* **Data analysis scripts** : documentation
* **Future prospects** : report and documentation


========================================
Getting to work on the project - 5 steps
========================================

If you are part of a new team working on this PRDW project, we suggest you proceed the following way :

1. Get used to the linux environment and the development server if you're not
2. Read & understand the scrapers' doc, and set it up if not already done
3. Read the project report to understand the technologic choices we have done for our Big Data architecture. You can begin to question it, but we advise you not to try a new architecture unless you've accomplished at least a bit of data analysis first
4. Get used to the pyspark shell and using our data analysis scripts in order to make your own in the future
5. GO AHEAD : think about new data mining algorithms, and implement it


How we size tasks on that project
---------------------------------

Here is a kind of calendar we would advise any new UTC group to follow :

* **Reading & uderstanding our project / setting it up / getting used to the different technologies** : 1 month
* **Studying and thinking new data analysis algorithms** : 2 months
* **Implementing these algorithms** : 1 month
* **MANDATORY / REALLY REALLY IMPORTANT documenting what they have done + final report** : 1 month


Good to know
------------

We have made backups of the datasets we have collected during our project. Feel free to email us so that we send it to you, or make it available for you.

